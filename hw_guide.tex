\documentclass{ee128lab}

\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,calc,fit}
\usepackage{graphicx}
\usepackage{booktabs}

% ---- Styles (tweak here to taste) ----
\tikzset{
  >=Latex,
  sig/.style={-Latex, thick},
  blk/.style={draw,rounded corners,thick,fill=white,
              minimum height=14mm,minimum width=29mm,align=center},
  gain/.style={draw,thick,fill=white,sharp corners,
              minimum height=14mm,minimum width=14mm,align=center},
  sum/.style={draw,circle,thick,inner sep=0pt,minimum size=4.5mm},
  tap/.style={circle,fill,inner sep=0.8pt},
  lab/.style={font=\footnotesize,inner sep=1pt}
}

% define the header
\lhead{EE128/221 Fall 2025 Lab 6}
\lfoot{Rev. 1.0, \today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{center}
\LARGE
\textbf{Lab 6: Reinforcement Learning for Rotary Inverted Pendulum Control}
\vspace{1ex}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5em}
\noindent\fbox{\parbox{0.95\textwidth}{
\textbf{Note:} The pre-lab requires significant computation time. Each training run takes approximately \textbf{1--2 hours} to complete. Plan accordingly and start early-you are asked to complete 3 training runs before the lab session so tt you can test them in the lab. Make sure to start the training early so that you have time to debug if something goes wrong.
}}
\vspace{0.5em}
\section{Objectives}
\label{Objectives:sec}

In this lab, you will apply \textbf{Reinforcement Learning (RL)} to control the Quanser Rotary Inverted Pendulum (ROTPEN). The objectives are:
\begin{itemize}
    \item Understand the RL problem formulation for swing-up and balance control
    \item Train an RL agent in simulation to perform the pendulum swing-up maneuver
    \item Explore the effect of hyperparameters on training performance
    \item Deploy trained policies on the physical hardware
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theory}
\label{Theory:sec}

\subsection{System Model}
\label{Theory:Model:subsec}

The Rotary Inverted Pendulum consists of a DC motor-driven rotary arm with a pendulum attached at its end, as shown in Figure~\ref{fig:rotpen}. The system has two degrees of freedom:
\begin{itemize}
    \item $\theta$: rotary arm angle (rad)
    \item $\phi$: pendulum angle from upright (rad), where $\phi = 0$ is the upright (unstable) equilibrium
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/rotpen.png}
    \caption{Rotary Inverted Pendulum schematic showing the rotary arm (length $r$), pendulum (length $L_p$), arm angle $\theta$, and pendulum angle $\phi$ (shown as $\alpha$ in the figure).}
    \label{fig:rotpen}
\end{figure}

The state vector is $\mathbf{x} = [\theta, \dot{\theta}, \phi, \dot{\phi}]^\top$, and the control input is the motor voltage $V_m$ (V).

Using the Euler-Lagrange method, the nonlinear equations of motion are:
\begin{align}
    &\left( m_p L_r^2 + \frac{1}{4}m_p L_p^2 - \frac{1}{4}m_p L_p^2 \cos^2(\phi) + J_r \right) \ddot{\theta} - \left( \frac{1}{2}m_p L_p L_r \cos(\phi) \right) \ddot{\phi} \notag \\
    &\quad + \left( \frac{1}{2}m_p L_p^2 \sin(\phi)\cos(\phi) \right) \dot{\theta}\dot{\phi} + \left( \frac{1}{2}m_p L_p L_r \sin(\phi) \right) \dot{\phi}^2 = \tau - B_r\dot{\theta} \label{eq:eom1} \\[1em]
    &-\frac{1}{2}m_p L_p L_r \cos(\phi)\ddot{\theta} + \left( J_p + \frac{1}{4}m_p L_p^2 \right) \ddot{\phi} - \frac{1}{4}m_p L_p^2 \cos(\phi)\sin(\phi)\dot{\theta}^2 \notag \\
    &\quad - \frac{1}{2}m_p L_p g \sin(\phi) = -B_p\dot{\phi} \label{eq:eom2}
\end{align}
where $m_p$ is the pendulum mass, $L_r$ is the arm length, $L_p$ is the pendulum length, $J_r$ and $J_p$ are moments of inertia, $B_r$ and $B_p$ are viscous damping coefficients, $g$ is gravitational acceleration, and $\tau$ is the applied torque.

The torque is generated by the servo motor:
\begin{equation}
    \tau = \frac{\eta_g K_g \eta_m k_t (V_m - K_g k_m \dot{\theta})}{R_m}
    \label{eq:torque}
\end{equation}
where $\eta_g$ is gearbox efficiency, $\eta_m$ is motor efficiency, $K_g$ is gear ratio, $k_t$ is motor torque constant, $k_m$ is back-EMF constant, and $R_m$ is motor resistance.

\textbf{Why Reinforcement Learning?} The coupled nonlinear dynamics in \eqref{eq:eom1}--\eqref{eq:eom2} make this system challenging to control using classical methods, especially for the swing-up maneuver which requires exploiting the system's natural dynamics. Rather than deriving complex analytical controllers, we use a simulation of this model to \emph{learn} control policies through reinforcement learning.

\subsection{Reinforcement Learning Formulation}
\label{Theory:RL:subsec}

In RL, an \textbf{agent} interacts with an \textbf{environment} over discrete time steps. At each step $t$, the agent:
\begin{enumerate}
    \item Observes state $s_t$
    \item Selects action $a_t$ according to policy $\pi(a|s)$
    \item Receives reward $r_t$ and transitions to next state $s_{t+1}$
\end{enumerate}

The goal is to learn a policy $\pi^*$ that maximizes the expected cumulative discounted reward:
\[
J(\pi) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
\]
where $\gamma \in [0,1)$ is the discount factor.

\subsubsection{Observations}
The observation vector provided to the agent is 7-dimensional:
\[
\mathbf{o}_t = [\sin(\theta), \cos(\theta), \dot{\theta}, \sin(\phi), \cos(\phi), \dot{\phi}, a_{t-1}]
\]
Using $\sin$ and $\cos$ of the angles avoids discontinuities at $\pm\pi$. Including the previous action $a_{t-1}$ helps the agent learn smooth control.

\subsubsection{Actions}
The action is the normalized motor voltage $a \in [-1, 1]$, which is scaled to the actual voltage range $[-V_{\max}, V_{\max}]$.

\subsubsection{Reward Function}
A well-designed reward function is critical for successful training. We use:
\begin{equation}
    r_t = 1 -\alpha \left( q_1 \theta^2 + q_2 \phi^2 + q_3 \dot{\theta}^2 + q_4 \dot{\phi}^2 + q_5 a_t^2 + q_6 (a_t - a_{t-1})^2 \right)
    \label{eq:reward}
\end{equation}
The reward weights are parameterized by the vector $\mathbf{q} = [q_1, q_2, q_3, q_4, q_5, q_6]$:
\begin{itemize}
    \item $q_1$: penalizes arm deviation from center
    \item $q_2$: penalizes pendulum deviation from upright ($\phi = 0$)
    \item $q_3, q_4$: penalize angular velocities
    \item $q_5$: penalizes control effort (motor voltage)
    \item $q_6$: penalizes control jerk (action changes), promoting smooth control
\end{itemize}

\subsubsection{Episode Termination}
An episode terminates when the arm angle exceeds a threshold:
\[
|\theta| > \theta_{\text{threshold}}
\]
This prevents the arm from spinning excessively and encourages the agent to keep the arm near the center.

\subsection{Soft Actor-Critic (SAC)}
\label{Theory:SAC:subsec}

SAC is an off-policy, actor-critic algorithm for continuous action spaces. We will be using SAC as it is one of the most popular and effective RL algorithms for continuous action spaces.  However you do not need to know the details of the algorithm, you can just use it as a black box. If you are interested in the details of the algorithm, here are the key features of SAC that might help you understand the algorithm better:

\begin{enumerate}
    \item \textbf{Maximum Entropy Objective}: SAC maximizes both expected return and policy entropy:
    \[
    J(\pi) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t \left( r_t + \beta \mathcal{H}(\pi(\cdot|s_t)) \right) \right]
    \]
    where $\mathcal{H}(\pi)$ is the entropy and $\beta$ (temperature) controls the exploration-exploitation trade-off.

    \item \textbf{Actor Network}: A neural network $\pi_\phi(a|s)$ outputs a stochastic policy (Gaussian distribution over actions).

    \item \textbf{Twin Critics}: Two Q-networks $Q_{\theta_1}(s,a)$ and $Q_{\theta_2}(s,a)$ estimate state-action values. Using the minimum of both reduces overestimation bias.

    \item \textbf{Experience Replay}: Past transitions are stored in a replay buffer and sampled uniformly for training, improving sample efficiency.

    \item \textbf{Automatic Temperature Tuning}: The temperature $\beta$ can be automatically adjusted to maintain a target entropy level.
\end{enumerate}

SAC is well-suited for robotic control tasks due to its sample efficiency, stability, and ability to handle continuous actions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pre-Lab}
\label{PreLab:sec}

\textbf{Goal}: Train an RL agent in simulation to perform the swing-up and balance maneuver.

\subsection{Setup}

\begin{enumerate}
    \item \textbf{Clone the repository}: Access the codebase at:
    
    \texttt{[GitHub Repository Link - TBD]}
    
    \item \textbf{Install dependencies}: Follow the installation instructions in the repository \texttt{README.md}. You will need MATLAB R2021a or later with the Reinforcement Learning Toolbox.
    
    \item \textbf{Verify installation}: Run the provided test script to confirm the environment and simulation work correctly.
\end{enumerate}

\subsection{Training with Default Parameters}

\begin{enumerate}
    \item Run the training script with default hyperparameters
    \item Monitor the training progress (episode rewards, episode length)
    \item Training may take 1--2 hours depending on hardware
    \item The agent should learn to swing up and balance the pendulum within 1000 episodes
\end{enumerate}

\subsection{Hyperparameter Exploration}

After completing the default training, explore the effect of different hyperparameters. Select parameters from Table~\ref{tab:hyperparams} and train additional agents.

\begin{table}[h]
\centering
\caption{SAC Hyperparameters for Exploration}
\label{tab:hyperparams}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Parameter} & \textbf{Default} & \textbf{Range to Explore} & \textbf{Effect} \\
\midrule
\multicolumn{4}{@{}l}{\textit{Algorithm Parameters}} \\
Learning Rate & $3 \times 10^{-4}$ & $10^{-4}$ -- $10^{-3}$ & Training speed/stability \\
Discount Factor ($\gamma$) & 0.99 & 0.95 -- 0.999 & Future reward weighting \\
Batch Size & 256 & 64 -- 512 & Gradient estimate variance \\
\midrule
\multicolumn{4}{@{}l}{\textit{Reward Weights $\mathbf{q} = [q_1, q_2, q_3, q_4, q_5, q_6]$}} \\
$q_1$ ($\theta^2$) & 1.0 & 0.5 -- 2.0 & Arm centering \\
$q_2$ ($\phi^2$) & 1.0 & 0.5 -- 5.0 & Pendulum upright \\
$q_3$ ($\dot{\theta}^2$) & 0.1 & 0.01 -- 0.5 & Arm velocity penalty \\
$q_4$ ($\dot{\phi}^2$) & 0.1 & 0.01 -- 0.5 & Pendulum velocity penalty \\
$q_5$ ($a_t^2$) & 0.01 & 0.001 -- 0.1 & Control effort \\
$q_6$ ($(a_t - a_{t-1})^2$) & 0.01 & 0.001 -- 0.1 & Control smoothness \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Deliverables}

\textbf{Before the lab session, complete the following:}

\begin{enumerate}
    \item \textbf{Train at least 3 agents}: 
    \begin{itemize}
        \item Run 1: Default parameters
        \item Run 2 and 3: Modified paramters of your choice (we suggest to change one paramter at a time, you can decide between your group members to change different parameters so that you can compare the results)
    \end{itemize}
    
    \item \textbf{Record training curves}: Save plots of episode reward vs.\ training episodes for each run.
    
    \item \textbf{Evaluate in simulation}: Test each trained policy and verify swing-up success. Record a video of successful swing-up behavior for each agent. Details are provided in the repository.
    
    \item \textbf{Save trained policies}: To save the trained agent without the large replay buffer and training statistics, run:
    \begin{verbatim}
    clear experience trainingStats
    save('experiment_1.mat')
    \end{verbatim}
    Repeat for each experiment (\texttt{experiment\_1.mat}, \texttt{experiment\_2.mat}, \texttt{experiment\_3.mat}).
    
    \item \textbf{Document observations}: Briefly note (1--2 sentences per run):
    \begin{itemize}
        \item How many training episodes until the agent learned swing-up?
        \item Did the agent achieve consistent balance?
        \item How did parameter changes affect learning?
    \end{itemize}
\end{enumerate}

\vspace{1em}
\noindent\fbox{\parbox{0.95\textwidth}{
\textbf{Bring to lab:}
\begin{itemize}
    \item 3 saved policy files (\texttt{experiment\_1.mat}, \texttt{experiment\_2.mat}, \texttt{experiment\_3.mat})
    \item 3 training curve plots
    \item 3 simulation videos showing swing-up behavior 
\end{itemize}
}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lab}
\label{Lab:sec} 

\textbf{DELIVER:}
\begin{itemize}
    \item 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis}
\label{Lab:Analysis:sec}

\begin{enumerate}
    \item 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}  
